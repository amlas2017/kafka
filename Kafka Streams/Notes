----> Kafka Streams est une API streaming transactionnelle. Elle permet de traiter les données unitairement, une à la fois directement depuis les topics Kafka.  

----> Kafka Streams comble parfaitement les limites d’un consumer classique. Là où un consumer ne sait que lire la donnée  sans pouvoir y effectuer des traitements 
réutilisables (par exemple : récupérer les données d’un topic, tester la validité des données, et publier les valides dans un autre topics, et les non-valides dans un 
autre), avec Kafka Streams, on peut appliquer des règles de validation à chaque donnée, et même effectuer des opérations complexes tels que les agrégations, 
les jointures, ou encore la gestion des cas complexes. 

----> Kafka Streams comble également les limites des « consumers décisionnels » tels que Spark Streaming ou Apache Flink. Là où ceux-ci sont obligés d’effectuer des 
traitements de données sur des micro-batch, avec Kafka Streams on effectue des traitements unitaires par défaut, et on a le choix entre ce mode de traitement et le 
traitement groupé. 

----> Là où la mise en œuvre de Storm demande la mise en place d’une architecture lambda complexe, l’utilisation de Kafka Streams nécessite juste un PC, car elle est une 
API très légère qui repose sur un cluster Kafka. Kafka Streams s’y connecte simplement comme client. 
C’est important de noter que même si Kafka Streams traite directement les topics sur Kafka, il ne tourne pas sur le cluster Kafka. Il tourne sur une machine standalone 
différente et le cluster Kafka le voit juste comme un Consumer qui s’est abonné à l’un de ses topics. 

---->  TP: https://www.youtube.com/watch?v=fACxTiCR-Ro


/////////////////********************************* Implementing Kafka Streams  *****************************************///////////////////////////

Basically, built with Kafka Streams, a stream processing application looks like:

a. Providing Stream Configurations

Properties streamsConfiguration = new Properties();
streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, “Streaming-QuickStart”);
streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, “localhost:9092”);
streamsConfiguration.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

b. Getting Topic and Serdes

String topic = configReader.getKStreamTopic();
String producerTopic = configReader.getKafkaTopic();
final Serde stringSerde = Serdes.String();
final Serde longSerde = Serdes.Long();

c. Building the Stream and Fetching Data

KStreamBuilder builder = new KStreamBuilder();
KStream<String, String> inputStreamData = builder.stream(stringSerde, stringSerde, producerTopic);

d. Processing of Kafka Stream

KStream<String, Long> processedStream = inputStreamData.mapValues(record -> record.length() )

e. Writing Streams Back to Kafka

processedStream.to(stringSerde, longSerde, topic);
KafkaStreams streams = new KafkaStreams(builder, streamsConfiguration);
streams.start();
